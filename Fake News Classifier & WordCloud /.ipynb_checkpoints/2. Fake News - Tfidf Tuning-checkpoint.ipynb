{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello World!\n",
    "In this project we're going to quickly load an dataset called Real_or_Fake_News and run a through a super simple classification modelling process. As we will see its an easy to use dataset with balanced labels and no missing data so no cleaning is required. The data itself consists of news headlines, articles and a finally a label indicting if its a real or fake news story. In this example I will just look at two versions of sklearn's tfidf, one baseline and one with tweaked parameters. Finally I will combine it with and a baseline Multinomial Naive Baye classifier then iterate over some alpha values. I have used train_test_split as a validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  6335 non-null   int64 \n",
      " 1   title       6335 non-null   object\n",
      " 2   text        6335 non-null   object\n",
      " 3   label       6335 non-null   object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.1+ KB\n",
      "None\n",
      "REAL    3171\n",
      "FAKE    3164\n",
      "Name: label, dtype: int64\n",
      "Unnamed: 0    0\n",
      "title         0\n",
      "text          0\n",
      "label         0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quick inspection of the data\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('fake_or_real_news.csv')\n",
    "\n",
    "print(df.info())\n",
    "print(df['label'].value_counts())\n",
    "print(df.isna().sum())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Create the labels\n",
    "y = df['label']\n",
    "\n",
    "# Create training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size=0.33, random_state=53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize two TfidfVectorizers object: basic and tuned \n",
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "tuned_tfidf_vect = TfidfVectorizer(stop_words='english', max_df=0.5,\n",
    "                                   max_features=25000, ngram_range=(2,2))\n",
    "\n",
    "# Transform the training data \n",
    "tfidf_train = tfidf_vect.fit_transform(X_train)\n",
    "\n",
    "tuned_tfidf_train = tuned_tfidf_vect.fit_transform(X_train)\n",
    "\n",
    "\n",
    "# Transform the test data\n",
    "tfidf_test = tfidf_vect.transform(X_test)\n",
    "\n",
    "tuned_tfidf_test = tuned_tfidf_vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrames for inspection\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vect.get_feature_names())\n",
    "\n",
    "tuned_tfidf_df = pd.DataFrame(tuned_tfidf_train.A, columns = tuned_tfidf_vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4244, 25000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_tfidf_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untuned Accuracy Score: 0.8283118125298901\n",
      "[[ 668  340]\n",
      " [  19 1064]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "# Instantiate the Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "# Fit the classifier to the untuned tfidf df\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags\n",
    "untuned_pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# Calculating the untuned accuracy score\n",
    "untuned_score = accuracy_score(y_test, untuned_pred)\n",
    "print('Untuned Accuracy Score:', untuned_score)\n",
    "\n",
    "# Confusion matrix\n",
    "untuned_cm = confusion_matrix(y_test, untuned_pred, labels =['FAKE', 'REAL'])\n",
    "print(untuned_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alomst 83%, not bad for a basic vectorizer and multinomial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Accuracy Score: 0.921090387374462\n",
      "[[ 896  112]\n",
      " [  53 1030]]\n"
     ]
    }
   ],
   "source": [
    "# Fit nb_classifier to the tuned model\n",
    "nb_classifier.fit(tuned_tfidf_train, y_train)\n",
    "\n",
    "# Create the predicted tags\n",
    "pred = nb_classifier.predict(tuned_tfidf_test)\n",
    "\n",
    "# Calculating the tuned accuracy score\n",
    "score = accuracy_score(y_test, pred)\n",
    "print('Tuned Accuracy Score:', score)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, pred, labels = [ 'FAKE', 'REAL'])\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost 10% higher accuracy at 92% by limiting the features and and setting a threshold on terms that occurred in too many documents (max_df). Interestingly the percision of the tuned vectorizer is a little more skewed having more false negatives, REAL news stories beging classed as FAKE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.9057867049258728\n",
      "\n",
      "\n",
      "Alpha:  0.05\n",
      "Score:  0.9196556671449068\n",
      "\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.9230033476805356\n",
      "\n",
      "\n",
      "Alpha:  0.15000000000000002\n",
      "Score:  0.9201339072214252\n",
      "\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.921090387374462\n",
      "\n",
      "\n",
      "Alpha:  0.25\n",
      "Score:  0.921090387374462\n",
      "\n",
      "\n",
      "Alpha:  0.30000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darragh/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.9220468675274988\n",
      "\n",
      "\n",
      "Alpha:  0.35000000000000003\n",
      "Score:  0.9225251076040172\n",
      "\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.9225251076040172\n",
      "\n",
      "\n",
      "Alpha:  0.45\n",
      "Score:  0.923481587757054\n",
      "\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.923481587757054\n",
      "\n",
      "\n",
      "Alpha:  0.55\n",
      "Score:  0.9239598278335724\n",
      "\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.9249163079866093\n",
      "\n",
      "\n",
      "Alpha:  0.65\n",
      "Score:  0.9244380679100909\n",
      "\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.9244380679100909\n",
      "\n",
      "\n",
      "Alpha:  0.75\n",
      "Score:  0.9244380679100909\n",
      "\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.9220468675274988\n",
      "\n",
      "\n",
      "Alpha:  0.8500000000000001\n",
      "Score:  0.9215686274509803\n",
      "\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.9215686274509803\n",
      "\n",
      "\n",
      "Alpha:  0.9500000000000001\n",
      "Score:  0.9215686274509803\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a function to iterate alpha values\n",
    "import numpy as np\n",
    "# Create list\n",
    "alphas = np.arange(0,1,0.05)\n",
    "\n",
    "def train_and_predict(alpha, X_train, y_train, tfidf, y_test):\n",
    "    nb_classifier = MultinomialNB(alpha =alpha)\n",
    "    nb_classifier.fit(X_train, y_train)\n",
    "    pred = nb_classifier.predict(tfidf)\n",
    "    score = accuracy_score(y_test, pred)\n",
    "    return score\n",
    "\n",
    "# Iterate over the alphas and print the corresponding score\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha, tuned_tfidf_train, y_train, tuned_tfidf_test, y_test))\n",
    "    print('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92.5% accuracy ith an Alpha of 0.6. Not bad for a basic classification analysis. \n",
    "\n",
    "Side note, we can actually get almost 91% when we use the untuned vectorizer and iterate over the alpha in MB bayes model. See below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alpha:  0.0\n",
      "Score:  0.8833094213295074\n",
      "\n",
      "\n",
      "Alpha:  0.05\n",
      "Score:  0.9043519846963175\n",
      "\n",
      "\n",
      "Alpha:  0.1\n",
      "Score:  0.899091343854615\n",
      "\n",
      "\n",
      "Alpha:  0.15000000000000002\n",
      "Score:  0.8952654232424677\n",
      "\n",
      "\n",
      "Alpha:  0.2\n",
      "Score:  0.8928742228598756\n",
      "\n",
      "\n",
      "Alpha:  0.25\n",
      "Score:  0.8919177427068389\n",
      "\n",
      "\n",
      "Alpha:  0.30000000000000004\n",
      "Score:  0.8876135820181731\n",
      "\n",
      "\n",
      "Alpha:  0.35000000000000003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darragh/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  0.8842659014825442\n",
      "\n",
      "\n",
      "Alpha:  0.4\n",
      "Score:  0.8790052606408417\n",
      "\n",
      "\n",
      "Alpha:  0.45\n",
      "Score:  0.8761358201817312\n",
      "\n",
      "\n",
      "Alpha:  0.5\n",
      "Score:  0.8703969392635102\n",
      "\n",
      "\n",
      "Alpha:  0.55\n",
      "Score:  0.8656145384983261\n",
      "\n",
      "\n",
      "Alpha:  0.6000000000000001\n",
      "Score:  0.8627450980392157\n",
      "\n",
      "\n",
      "Alpha:  0.65\n",
      "Score:  0.8574844571975132\n",
      "\n",
      "\n",
      "Alpha:  0.7000000000000001\n",
      "Score:  0.8531802965088474\n",
      "\n",
      "\n",
      "Alpha:  0.75\n",
      "Score:  0.8474414155906265\n",
      "\n",
      "\n",
      "Alpha:  0.8\n",
      "Score:  0.8407460545193687\n",
      "\n",
      "\n",
      "Alpha:  0.8500000000000001\n",
      "Score:  0.8393113342898135\n",
      "\n",
      "\n",
      "Alpha:  0.9\n",
      "Score:  0.8335724533715926\n",
      "\n",
      "\n",
      "Alpha:  0.9500000000000001\n",
      "Score:  0.8311812529890005\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Untuned tfidf\n",
    "for alpha in alphas:\n",
    "    print('Alpha: ', alpha)\n",
    "    print('Score: ', train_and_predict(alpha, tfidf_train, y_train, tfidf_test, y_test))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the top most positive and negative correlatated features for each class for both tuned and untuned vectorizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNTUNED FAKE [(-10.621740722043343, '00'), (-10.621740722043343, '000km'), (-10.621740722043343, '00p'), (-10.621740722043343, '01'), (-10.621740722043343, '02')] \n",
      "\n",
      "TUNED FAKE [(-10.621740722043343, '00 00'), (-10.621740722043343, '000 civilians'), (-10.621740722043343, '000 ounces'), (-10.621740722043343, '000 pageviews'), (-10.621740722043343, '000 soldiers')] \n",
      "\n",
      "\n",
      " UNTUNED REAL [(-7.09885061028009, 'handheld'), (-7.093119440450414, 'chiles'), (-7.088243482716475, 'desiree'), (-6.975711474893024, 'hnrca'), (-6.899553323905761, 'blogosphere')]\n",
      "\n",
      " TUNED REAL [(-7.09885061028009, 'united states'), (-7.093119440450414, 'hillary clinton'), (-7.088243482716475, 'new york'), (-6.975711474893024, 'white house'), (-6.899553323905761, 'donald trump')]\n"
     ]
    }
   ],
   "source": [
    "# Get the class labels FAKE and REAL\n",
    "class_labels = nb_classifier.classes_\n",
    "\n",
    "# Feature_names\n",
    "feature_names = tfidf_vect.get_feature_names()\n",
    "tuned_feature_names = tuned_tfidf_vect.get_feature_names()\n",
    "\n",
    "# Sort and zip feature names with the coefficients\n",
    "feat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n",
    "tuned_feat_with_weights = sorted(zip(nb_classifier.coef_[0], tuned_feature_names))\n",
    "\n",
    "\n",
    "\n",
    "print('UNTUNED', class_labels[0], feat_with_weights[:5],'\\n')\n",
    "print('TUNED', class_labels[0], tuned_feat_with_weights[:5],'\\n')\n",
    "\n",
    "\n",
    "print('\\n', 'UNTUNED',  class_labels[1], feat_with_weights[-5:])\n",
    "print('\\n', 'TUNED', class_labels[1], tuned_feat_with_weights[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally, lets quickly look at the accuracy from just the headlines, and then the headline + article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.8976566236250598\n",
      "\n",
      "\n",
      "0.05\n",
      "0.9206121472979436\n",
      "\n",
      "\n",
      "0.1\n",
      "0.9186991869918699\n",
      "\n",
      "\n",
      "0.15000000000000002\n",
      "0.9201339072214252\n",
      "\n",
      "\n",
      "0.2\n",
      "0.9230033476805356\n",
      "\n",
      "\n",
      "0.25\n",
      "0.923481587757054\n",
      "\n",
      "\n",
      "0.30000000000000004\n",
      "0.9230033476805356\n",
      "\n",
      "\n",
      "0.35000000000000003\n",
      "0.9215686274509803\n",
      "\n",
      "\n",
      "0.4\n",
      "0.9225251076040172\n",
      "\n",
      "\n",
      "0.45\n",
      "0.923481587757054\n",
      "\n",
      "\n",
      "0.5\n",
      "0.923481587757054\n",
      "\n",
      "\n",
      "0.55\n",
      "0.9225251076040172\n",
      "\n",
      "\n",
      "0.6000000000000001\n",
      "0.9230033476805356\n",
      "\n",
      "\n",
      "0.65"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darragh/opt/anaconda3/lib/python3.7/site-packages/sklearn/naive_bayes.py:508: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10\n",
      "  'setting alpha = %.1e' % _ALPHA_MIN)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.9244380679100909\n",
      "\n",
      "\n",
      "0.7000000000000001\n",
      "0.9244380679100909\n",
      "\n",
      "\n",
      "0.75\n",
      "0.9239598278335724\n",
      "\n",
      "\n",
      "0.8\n",
      "0.9225251076040172\n",
      "\n",
      "\n",
      "0.8500000000000001\n",
      "0.9215686274509803\n",
      "\n",
      "\n",
      "0.9\n",
      "0.9201339072214252\n",
      "\n",
      "\n",
      "0.9500000000000001\n",
      "0.9196556671449068\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df['title_text'] = df['title']+ ' '+df['text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['title_text'], y, test_size=0.33, random_state = 42)\n",
    "\n",
    "tuned_header_text_tfidf_train = tuned_tfidf_vect.fit_transform(X_train)\n",
    "\n",
    "tuned_header_text_tfidf_test = tuned_tfidf_vect.transform(X_test)\n",
    "\n",
    "for alpha in alphas:\n",
    "    print(alpha)\n",
    "    print(train_and_predict(alpha, tuned_header_text_tfidf_train, y_train, tuned_header_text_tfidf_test, y_test))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesnt look like we're getting higher than 92.5% without diving into more complex df processing and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
